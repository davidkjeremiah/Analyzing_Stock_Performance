{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extracting Stock Data Using a Web Scraping**\n",
    "\n",
    "Created on Wed Aug 3 02:56:02 2022\n",
    "\n",
    "@author: David K. Jeremiah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objectives**\n",
    "\n",
    "Not all stock data is available via API, some stock info or financial data can only be obtained using web-scraping.\n",
    "\n",
    "Using beautiful soup we will extract historical share data from a web-page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "* Downloading the Webpage Using Requests Library\n",
    "* Parsing Webpage HTML Using BeautifulSoup\n",
    "* Extracting Data and Building DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using Webscraping to Extract Stock Data for Netflix**\n",
    "First we use the `request` library to downlaod the webpage, and extract the text. We will extract `Netflix` [stock data](https://finance.yahoo.com/quote/NFLX/history?p=NFLX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html\"\n",
    "\n",
    "response = requests.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must parse the text into `html` using beautiful_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can turn the html table into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_data = pd.DataFrame(columns=[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"])\n",
    "\n",
    "# First we isolate the body of the table which contains all the information\n",
    "# Then we loop through each row and find all the column values for each row\n",
    "for row in soup.find(\"tbody\").find_all('tr'):\n",
    "    col = row.find_all(\"td\")\n",
    "    date = col[0].text\n",
    "    Open = col[1].text\n",
    "    high = col[2].text\n",
    "    low = col[3].text\n",
    "    close = col[4].text\n",
    "    adj_close = col[5].text\n",
    "    volume = col[6].text\n",
    "    \n",
    "    # Finally we append the data of each row to the table\n",
    "    netflix_data = netflix_data.append({\"Date\":date, \"Open\":Open, \"High\":high, \"Low\":low, \"Close\":close, \"Adj Close\":adj_close, \"Volume\":volume}, ignore_index=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print out the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date    Open    High     Low   Close Adj Close       Volume\n",
      "0  Jun 01, 2021  504.01  536.13  482.14  528.21    528.21   78,560,600\n",
      "1  May 01, 2021  512.65  518.95  478.54  502.81    502.81   66,927,600\n",
      "2  Apr 01, 2021  529.93  563.56  499.00  513.47    513.47  111,573,300\n",
      "3  Mar 01, 2021  545.57  556.99  492.85  521.66    521.66   90,183,900\n",
      "4  Feb 01, 2021  536.79  566.65  518.28  538.85    538.85   61,902,300\n"
     ]
    }
   ],
   "source": [
    "print(netflix_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the pandas `read_html` function using the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_html_pandas_data = pd.read_html(url)\n",
    "\n",
    "# print(read_html_pandas_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can convert the BeautifulSoup object to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_html_pandas_data = pd.read_html(str(soup))\n",
    "\n",
    "# print(read_html_pandas_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beacause there is only one table on the page, we just take the first table in the list returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date    Open    High     Low  Close* Adj Close**     Volume\n",
      "0  Jun 01, 2021  504.01  536.13  482.14  528.21      528.21   78560600\n",
      "1  May 01, 2021  512.65  518.95  478.54  502.81      502.81   66927600\n",
      "2  Apr 01, 2021  529.93  563.56  499.00  513.47      513.47  111573300\n",
      "3  Mar 01, 2021  545.57  556.99  492.85  521.66      521.66   90183900\n",
      "4  Feb 01, 2021  536.79  566.65  518.28  538.85      538.85   61902300\n"
     ]
    }
   ],
   "source": [
    "netflix_data = read_html_pandas_data[0]\n",
    "\n",
    "print(netflix_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using Webscraping to Extract Stock Data for Amazon**\n",
    "First, we use the `requests` library to download the [webpage](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_ = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html\"\n",
    "\n",
    "html_data = requests.get(url_).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the html data using beautiful_soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_ = BeautifulSoup(html_data, 'html.parser')\n",
    "\n",
    "# print(soup_.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be sure we are considering the correct stock's (i.e. Amazon) Historical Prices & Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon.com, Inc. (AMZN) Stock Historical Prices & Data - Yahoo Finance\n"
     ]
    }
   ],
   "source": [
    "title = soup_.title.text\n",
    "\n",
    "# print content of the title attribute\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using beautiful soup, we extract the table with historical share prices and store it into a dataframe named `amazon_data`. \n",
    "\n",
    "The dataframe would have columns `Date`, `Open`, `High`, `Low`, `Close`, `Adj Close`, and `Volume`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas for Amazon stock\n",
    "amazon_data = pd.DataFrame(columns=['Date', 'Open', 'High', 'Low', 'Close', 'Adj close', 'Volumne'])\n",
    "\n",
    "# parsing html\n",
    "# first isolate the table body\n",
    "# Then we loop through each row and find all the column values for each row\n",
    "for row in soup_.find('tbody').find_all('tr'):\n",
    "    col = row.find_all('td')\n",
    "    date = col[0].text\n",
    "    open = col[1].text\n",
    "    high = col[2].text\n",
    "    low = col[3].text\n",
    "    close = col[4].text\n",
    "    adj_close = col[5].text\n",
    "    volume = col[6].text\n",
    "    \n",
    "    # Finally we append the data of each row to the table\n",
    "    amazon_data = amazon_data.append({'Date':date, 'Open':open, 'High':high, 'Low':low, 'Close':close, 'Adj close':adj_close, 'Volumne':volume}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date      Open      High       Low     Close Adj close      Volumne\n",
      "0  Jan 01, 2021  3,270.00  3,363.89  3,086.00  3,206.20  3,206.20   71,528,900\n",
      "1  Dec 01, 2020  3,188.50  3,350.65  3,072.82  3,256.93  3,256.93   77,556,200\n",
      "2  Nov 01, 2020  3,061.74  3,366.80  2,950.12  3,168.04  3,168.04   90,810,500\n",
      "3  Oct 01, 2020  3,208.00  3,496.24  3,019.00  3,036.15  3,036.15  116,226,100\n",
      "4  Sep 01, 2020  3,489.58  3,552.25  2,871.00  3,148.73  3,148.73  115,899,300\n"
     ]
    }
   ],
   "source": [
    "# Print out the first five rows of the amazon_data dataframe you created.\n",
    "print(amazon_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using Webscraping to Extract Stock Data for Meta - a shortcut method**\n",
    "First, we get the url of [Meta's](https://about.facebook.com/) (META) Stock Historical Prices Data from [Yahoo Finance](https://query1.finance.yahoo.com/v7/finance/download/META?period1=1628004714&period2=1659540714&interval=1d&events=history&includeAdjustedClose=true). In this case, we are considering the historical data within the following `Time Period:` Aug 03, 2021 - Aug 03, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://query1.finance.yahoo.com/v7/finance/download/META?period1=1628004714&period2=1659540714&interval=1d&events=history&includeAdjustedClose=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using pandas DataFrame method, we read the url, which is in `CSV` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date        Open        High         Low       Close   Adj Close  \\\n",
      "0  2021-08-03  352.730011  353.769989  347.700012  351.239990  351.239990   \n",
      "1  2021-08-04  352.420013  360.480011  351.510010  358.920013  358.920013   \n",
      "2  2021-08-05  359.640015  363.899994  356.899994  362.970001  362.970001   \n",
      "3  2021-08-06  361.399994  365.149994  361.399994  363.510010  363.510010   \n",
      "4  2021-08-09  363.760010  365.779999  360.750000  361.609985  361.609985   \n",
      "\n",
      "     Volume  \n",
      "0  12406100  \n",
      "1  14180600  \n",
      "2  10247200  \n",
      "3   8925000  \n",
      "4   7798900  \n"
     ]
    }
   ],
   "source": [
    "meta_data = pd.read_csv(url)\n",
    "\n",
    "# print the first five rows of the data\n",
    "print(meta_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the current stock price of META as at `2022-08-03` for the entire day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date        Open        High         Low       Close   Adj Close  \\\n",
      "252  2022-08-03  162.690002  169.539993  161.570007  168.800003  168.800003   \n",
      "\n",
      "       Volume  \n",
      "252  31721900  \n"
     ]
    }
   ],
   "source": [
    "print(meta_data[meta_data['Date'] == '2022-08-03'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that generally, the [Bull market](https://www.investopedia.com/insights/digging-deeper-bull-and-bear-markets/#:~:text=Bull%20Market%20vs.-,Bear%20Market,stocks%20are%20declining%20in%20value) triumphed over the [Bear market](https://www.investopedia.com/insights/digging-deeper-bull-and-bear-markets/#:~:text=Bull%20Market%20vs.-,Bear%20Market,stocks%20are%20declining%20in%20value), given that the `close price` an the end of the day was higher (very close to the high price) than the `opening price`. This indicates that thus far, Meta stock isn't undervalued."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "830c937019861666cce61b402dd489df1159c5110f29728a226ccbc42880e38c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
